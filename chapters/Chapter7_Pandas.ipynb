{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Pandas - The Workhorse of Data Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## The CRAWL â†’ WALK â†’ RUN Framework\n",
    "\n",
    "This textbook uses a structured approach to learning Python while developing effective AI collaboration skills. Each chapter follows three distinct phases:\n",
    "\n",
    "| Mode | Icon | AI Policy | Purpose |\n",
    "|------|------|-----------|--------|\n",
    "| **CRAWL** | ðŸ› | No AI assistance | Build foundational skills you can demonstrate independently |\n",
    "| **WALK** | ðŸš¶ | AI for understanding only | Use AI to explain concepts and errors, but write your own code |\n",
    "| **RUN** | ðŸš€ | Full AI collaboration | Partner with AI on complex tasks while documenting your process |\n",
    "\n",
    "**Why This Matters:** Your final exam will test CRAWL and WALK material with no AI assistance. If you skip the foundational work and rely entirely on AI, you won't pass. The progression ensures you build genuine competence before leveraging AI as a professional tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸ“Š Case Study Continues: From Arrays to DataFrames\n\nIn Chapter 6, you analyzed the Crestview student dataset using NumPy. You wrote code like this:\n\n```python\n# NumPy approach: grouped analysis\nfor year in sorted(unique_years):\n    mask = np.array([y == year for y in class_years])\n    year_gpas = gpas[mask]\n    print(f\"{year}: {np.mean(year_gpas):.3f}\")\n```\n\nThat's five lines of code. Here's the pandas version:\n\n```python\ndf.groupby('Class_Year')['GPA'].mean()\n```\n\nOne line. Same result.\n\n**Pandas doesn't replace NumPy.** Pandas is built on top of NumPy. DataFrames store their data internally as NumPy arrays. What pandas adds is:\n\n| NumPy Arrays | Pandas DataFrames |\n|-------------|------------------|\n| Positional indexing only (0, 1, 2...) | Label-based indexing ('GPA', 'College') |\n| Homogeneous data types | Mixed types per column |\n| Great for numerical computation | Great for tabular data with mixed types |\n| Manual group-by with boolean masks | Built-in groupby, merge, pivot operations |\n| No built-in missing value handling | Sophisticated NaN handling |\n\n**The Professional Reality:**\n\nIn industry, you'll spend 60-80% of your time loading, cleaning, transforming, and exploring data. Pandas is the tool for that work. NumPy is for the numerical heavy lifting once your data is ready.\n\n**Continuing with Our Dataset:**\n\nWe'll use the **extended Crestview student dataset** (600 students, 15 variables) throughout this chapter. After mastering data cleaning in Chapter 5, you earned access to this richer dataset â€” the same 7 columns you already know, plus 8 new variables including Study_Hours_Per_Week, Campus_Housing, Financial_Aid, Extracurriculars, Part_Time_Job, First_Generation, and Distance_From_Home. By the end of this chapter, you'll be able to answer multi-dimensional questions about students that would have taken dozens of lines in NumPy."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will:\n",
    "\n",
    "- ðŸ› Load CSV files into DataFrames using `pd.read_csv()`\n",
    "- ðŸ› Inspect DataFrames with `head()`, `info()`, `describe()`, and `shape`\n",
    "- ðŸ› Select columns using bracket notation and dot notation\n",
    "- ðŸ› Select rows using `loc[]` (label-based) and `iloc[]` (position-based)\n",
    "- ðŸ› Filter rows using boolean conditions\n",
    "- ðŸ› Sort DataFrames by one or more columns\n",
    "- ðŸš¶ Use `groupby()` for split-apply-combine operations\n",
    "- ðŸš¶ Create new columns from existing data\n",
    "- ðŸš¶ Handle missing values with `isna()`, `dropna()`, and `fillna()`\n",
    "- ðŸš¶ Merge and concatenate DataFrames\n",
    "- ðŸš€ Build a complete data analysis pipeline from raw data to insights\n",
    "- ðŸš€ Recognize when AI-generated pandas code needs correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ› CRAWL: Pandas Fundamentals\n",
    "\n",
    "**Rules for this section:**\n",
    "- Close all AI tools (ChatGPT, Claude, Copilot, etc.)\n",
    "- Work through examples by typing them yourself\n",
    "- Use only this notebook, Python documentation, or your instructor for help\n",
    "- This material will appear on the final exam without AI assistance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š DataCamp Resources for Chapter 7\n",
    "\n",
    "**[Data Manipulation with pandas](https://www.datacamp.com/courses/data-manipulation-with-pandas)** - Complete these:\n",
    "\n",
    "| Chapter | Topics Covered | Alignment |\n",
    "|---------|---------------|------------|\n",
    "| Chapter 1: Transforming DataFrames | Inspecting, sorting, subsetting | Sections 7.1-7.4 |\n",
    "| Chapter 2: Aggregating DataFrames | Summary statistics, groupby | Sections 7.5-7.6 |\n",
    "| Chapter 3: Slicing and Indexing | loc, iloc, pivot tables | Sections 7.3-7.4 |\n",
    "| Chapter 4: Creating and Visualizing DataFrames | New columns, plotting | Section 7.8 |\n",
    "\n",
    "**[Joining Data with pandas](https://www.datacamp.com/courses/joining-data-with-pandas)** - Complete:\n",
    "\n",
    "| Chapter | Topics Covered | Alignment |\n",
    "|---------|---------------|------------|\n",
    "| Chapter 1: Data Merging Basics | Inner, left, right joins | Section 7.9 |\n",
    "| Chapter 2: Merging Tables | Concatenation, validation | Section 7.9 |\n",
    "\n",
    "**Estimated time:** 6-8 hours total\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Introduction to Pandas and DataFrames\n",
    "\n",
    "Pandas has two primary data structures:\n",
    "\n",
    "- **Series:** A one-dimensional labeled array (like a single column)\n",
    "- **DataFrame:** A two-dimensional labeled data structure (like a spreadsheet or SQL table)\n",
    "\n",
    "Let's start by importing pandas and loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas (conventional alias: pd)\n",
    "import pandas as pd\n",
    "import numpy as np  # We'll still use NumPy occasionally\n",
    "\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the Crestview student dataset (extended version: 15 variables)\ndf = pd.read_csv('data/crestview_students_extended.csv')\n\n# What type is df?\nprint(f\"Type: {type(df)}\")\nprint(f\"Columns: {df.columns.tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "That's it. One line to load 600 records with 15 columns. Compare that to the Chapter 5 approach with `csv.DictReader` and loops."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Inspecting DataFrames\n",
    "\n",
    "Before doing any analysis, you need to understand your data. Pandas provides several methods for quick inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 rows (default)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions: (rows, columns)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Number of students: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data types and memory usage\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Notice the `Dtype` column. Pandas has inferred:\n- `object` for text columns (Student_ID, College, Major, Class_Year, Campus_Housing, Financial_Aid, Part_Time_Job, First_Generation, Enrollment_Date)\n- `float64` for GPA, Study_Hours_Per_Week, and Distance_From_Home\n- `int64` for Credits_Attempted, Credits_Earned, and Extracurriculars\n\nThis automatic type inference is one of pandas' conveniences. With 15 columns of mixed types, imagine having to specify each one manually!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "`describe()` gives you the same statistics you calculated manually with NumPy in Chapter 6 â€” but now for all numeric columns at once: GPA, Credits_Attempted, Credits_Earned, Study_Hours_Per_Week, Extracurriculars, and Distance_From_Home. This includes count, mean, std, min, 25th/50th/75th percentiles, and max."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include non-numeric columns in summary\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For non-numeric columns, you get `count`, `unique` (number of distinct values), `top` (most common value), and `freq` (frequency of most common)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Practice 7.1: DataFrame Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How many unique colleges are in the dataset?\n",
    "# Hint: Use df['College'].nunique() or len(df['College'].unique())\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the most common major in the dataset?\n",
    "# Hint: Use df['Major'].value_counts()\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What is the average GPA in the dataset?\n",
    "# Hint: Use df['GPA'].mean()\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How many students are in each class year?\n",
    "# Hint: Use df['Class_Year'].value_counts()\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Selecting Data: Columns\n",
    "\n",
    "There are two main ways to select columns: bracket notation and dot notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bracket notation: returns a Series\n",
    "gpas = df['GPA']\n",
    "print(f\"Type: {type(gpas)}\")\n",
    "print(gpas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot notation: also returns a Series (when column name is a valid identifier)\n",
    "gpas = df.GPA\n",
    "print(f\"Type: {type(gpas)}\")\n",
    "print(gpas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use each:**\n",
    "\n",
    "| Bracket Notation | Dot Notation |\n",
    "|-----------------|-------------|\n",
    "| Works for any column name | Only works if column name is a valid Python identifier |\n",
    "| Required for column names with spaces | Can't use for `df.Class Year` (invalid syntax) |\n",
    "| Required for column names matching DataFrame methods | Can't use `df.count` if there's a column named 'count' |\n",
    "| Allows selecting multiple columns | Only selects one column |\n",
    "\n",
    "**Recommendation:** Use bracket notation for consistency and to avoid surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns: pass a list of column names\n",
    "# Returns a DataFrame (not a Series)\n",
    "subset = df[['Student_ID', 'GPA', 'Major']]\n",
    "print(f\"Type: {type(subset)}\")\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Series is like a single column with an index\n",
    "gpa_series = df['GPA']\n",
    "print(f\"Shape: {gpa_series.shape}\")  # 1D: just length\n",
    "print(f\"Index: {gpa_series.index[:5].tolist()}\")  # Row labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do NumPy operations on a Series\n",
    "print(f\"Mean GPA: {gpa_series.mean():.3f}\")\n",
    "print(f\"Std GPA: {gpa_series.std():.3f}\")\n",
    "print(f\"GPAs above 3.5: {(gpa_series > 3.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Selecting Data: Rows\n",
    "\n",
    "Pandas provides two main indexers for row selection:\n",
    "\n",
    "- **`loc[]`**: Label-based indexing (uses row/column names)\n",
    "- **`iloc[]`**: Integer position-based indexing (uses 0, 1, 2...)\n",
    "\n",
    "This is one of the most confusing parts of pandas for beginners. Take your time here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our DataFrame has a default integer index (0, 1, 2, ...)\n",
    "print(f\"Index type: {type(df.index)}\")\n",
    "print(f\"First 5 index values: {df.index[:5].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc: Integer position based\n",
    "# Get the first row (position 0)\n",
    "first_student = df.iloc[0]\n",
    "print(\"First student (iloc[0]):\")\n",
    "print(first_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc: Get rows 0, 1, 2 (like Python slicing)\n",
    "first_three = df.iloc[0:3]\n",
    "first_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc: Get specific rows and columns by position\n",
    "# Rows 0-2, columns 0-2 (Student_ID, College, Major)\n",
    "df.iloc[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc: Label-based indexing\n",
    "# When index is 0, 1, 2... loc and iloc look similar\n",
    "# But loc uses the INDEX LABEL, not position\n",
    "\n",
    "# Get row with index label 0\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc with column names\n",
    "# Get rows 0-2, columns 'Student_ID' through 'Major'\n",
    "df.loc[0:2, 'Student_ID':'Major']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical Difference:** Notice that `loc[0:2]` includes row 2, while `iloc[0:3]` excludes position 3. This is because:\n",
    "- `iloc` uses Python-style slicing (exclusive end)\n",
    "- `loc` uses label-style slicing (inclusive end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference becomes clearer with a non-integer index\n",
    "# Set Student_ID as the index\n",
    "df_indexed = df.set_index('Student_ID')\n",
    "df_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now loc uses the Student_ID labels\ndf_indexed.loc['CU100001']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc still uses integer positions\n",
    "df_indexed.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index back to default\n",
    "df = df_indexed.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Filtering Rows with Boolean Conditions\n",
    "\n",
    "The most common way to select rows is with boolean conditions. This works like NumPy boolean masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean Series\n",
    "high_gpa = df['GPA'] >= 3.5\n",
    "print(f\"Type: {type(high_gpa)}\")\n",
    "print(high_gpa.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use boolean Series to filter rows\n",
    "honors_students = df[high_gpa]\n",
    "print(f\"Number of honors students (GPA >= 3.5): {len(honors_students)}\")\n",
    "honors_students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or in one line (more common)\n",
    "honors_students = df[df['GPA'] >= 3.5]\n",
    "print(f\"Honors students: {len(honors_students)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions: use & (and), | (or), ~ (not)\n",
    "# MUST use parentheses around each condition!\n",
    "\n",
    "# Students with GPA >= 3.5 AND in College of Business\n",
    "business_honors = df[(df['GPA'] >= 3.5) & (df['College'] == 'College of Business')]\n",
    "print(f\"Business honors students: {len(business_honors)}\")\n",
    "business_honors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students on probation (GPA < 2.0) OR seniors with low completion rate\n",
    "at_risk = df[(df['GPA'] < 2.0) | \n",
    "             ((df['Class_Year'] == 'Senior') & \n",
    "              (df['Credits_Earned'] / df['Credits_Attempted'] < 0.85))]\n",
    "print(f\"At-risk students: {len(at_risk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter using .isin() for multiple values\n# Students in Engineering or Business\nselected_colleges = ['College of Engineering', 'College of Business']\nengineering_business = df[df['College'].isin(selected_colleges)]\nprint(f\"Engineering and Business students: {len(engineering_business)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using string methods\n",
    "# Majors containing \"Engineering\"\n",
    "engineering_majors = df[df['Major'].str.contains('Engineering')]\n",
    "print(f\"Engineering majors: {len(engineering_majors)}\")\n",
    "print(f\"Unique engineering majors: {engineering_majors['Major'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Practice 7.2: Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How many students are Juniors or Seniors?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the average GPA of Finance majors?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How many students have earned more than 100 credits AND have a GPA above 3.0?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Find all students whose major starts with \"Computer\"\n",
    "# Hint: Use .str.startswith()\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Sorting Data\n",
    "\n",
    "Use `sort_values()` to sort by one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by GPA (ascending by default)\n",
    "df.sort_values('GPA').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by GPA descending (highest first)\n",
    "df.sort_values('GPA', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns\n",
    "# First by College (A-Z), then by GPA (highest first) within each college\n",
    "df.sort_values(['College', 'GPA'], ascending=[True, False]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 students by GPA\n",
    "top_10 = df.sort_values('GPA', ascending=False).head(10)\n",
    "top_10[['Student_ID', 'Major', 'GPA']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `sort_values()` returns a new DataFrame by default. The original is unchanged. To modify in place, use `inplace=True` (though this is generally discouraged in modern pandas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Aggregation Functions\n",
    "\n",
    "Pandas provides the same aggregation functions as NumPy, plus some extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregations on a Series\n",
    "print(f\"Mean GPA: {df['GPA'].mean():.3f}\")\n",
    "print(f\"Median GPA: {df['GPA'].median():.3f}\")\n",
    "print(f\"Std GPA: {df['GPA'].std():.3f}\")\n",
    "print(f\"Min GPA: {df['GPA'].min():.3f}\")\n",
    "print(f\"Max GPA: {df['GPA'].max():.3f}\")\n",
    "print(f\"Sum Credits: {df['Credits_Earned'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and unique\n",
    "print(f\"Total students: {df['Student_ID'].count()}\")\n",
    "print(f\"Unique colleges: {df['College'].nunique()}\")\n",
    "print(f\"Unique majors: {df['Major'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts: frequency of each value\n",
    "df['College'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts as percentages\n",
    "df['College'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations at once using agg()\n",
    "df['GPA'].agg(['mean', 'std', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations on multiple columns\n",
    "df[['GPA', 'Credits_Attempted', 'Credits_Earned']].agg(['mean', 'std', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Practice 7.3: Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What percentage of students are Seniors?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the median Credits_Earned?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Which major has the most students? How many?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What is the 75th percentile for GPA?\n",
    "# Hint: Use .quantile(0.75)\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CRAWL Checkpoint: Key Concepts So Far\n",
    "\n",
    "Before moving on, make sure you can do these **without AI assistance**:\n",
    "\n",
    "1. Load a CSV into a DataFrame\n",
    "2. Inspect with `head()`, `info()`, `describe()`, `shape`\n",
    "3. Select columns: `df['column']` or `df[['col1', 'col2']]`\n",
    "4. Filter rows: `df[df['GPA'] > 3.5]`\n",
    "5. Combine conditions: `df[(condition1) & (condition2)]`\n",
    "6. Sort: `df.sort_values('column', ascending=False)`\n",
    "7. Aggregate: `df['column'].mean()`, `.sum()`, `.value_counts()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRAWL Exercises\n",
    "\n",
    "Complete these exercises without AI assistance. They test the material covered so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7.1: Code Prediction\n",
    "\n",
    "Predict the output of each code snippet **before** running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will this return? (Type, number of columns)\n",
    "result = df['GPA']\n",
    "# Your prediction: \n",
    "\n",
    "# Now check:\n",
    "print(type(result), result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will this return? (Type, number of columns)\n",
    "result = df[['GPA']]\n",
    "# Your prediction: \n",
    "\n",
    "# Now check:\n",
    "print(type(result), result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this expression evaluate to?\n",
    "result = (df['GPA'] > 3.5).sum()\n",
    "# Your prediction (what kind of value?):\n",
    "\n",
    "# Now check:\n",
    "print(result, type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7.2: Debug These Errors\n",
    "\n",
    "Each cell contains an error. Fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 1: This should select students with GPA between 3.0 and 3.5\n",
    "middle_gpa = df[df['GPA'] >= 3.0 and df['GPA'] <= 3.5]\n",
    "middle_gpa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 2: This should select the GPA and Major columns\n",
    "subset = df['GPA', 'Major']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error 3: This should filter for Computer Science majors\n",
    "cs_students = df[df['Major'] = 'Computer Science']\n",
    "cs_students.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Problem 7.3: Analysis Questions\n\nAnswer these questions about the Crestview dataset using pandas."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) What is the GPA of the student with the most credits earned?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) How many students have a perfect 4.0 GPA?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) What is the most common Class_Year among students with GPA < 2.0?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) List the top 5 majors by average GPA (show major and average GPA)\n",
    "# Hint: You'll need groupby, which is covered in WALK, but try filtering first\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸš¶ WALK: GroupBy, New Columns, and Missing Data\n",
    "\n",
    "**Rules for this section:**\n",
    "- You may use AI tools to **explain** concepts and errors\n",
    "- You must **write all code yourself**\n",
    "- Good prompts: \"Explain how pandas groupby works\" or \"What's the difference between transform and apply?\"\n",
    "- Bad prompts: \"Write code that calculates mean GPA by college\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 GroupBy: Split-Apply-Combine\n",
    "\n",
    "The `groupby()` method is one of pandas' most powerful features. It implements the **split-apply-combine** pattern:\n",
    "\n",
    "1. **Split:** Divide data into groups based on some criteria\n",
    "2. **Apply:** Apply a function to each group independently\n",
    "3. **Combine:** Combine results into a data structure\n",
    "\n",
    "This replaces the manual loop-and-mask approach from NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average GPA by College\n",
    "# NumPy approach (Chapter 6):\n",
    "# for college in unique_colleges:\n",
    "#     mask = np.array([c == college for c in colleges])\n",
    "#     print(f\"{college}: {np.mean(gpas[mask]):.3f}\")\n",
    "\n",
    "# Pandas approach:\n",
    "df.groupby('College')['GPA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The groupby object itself is a container\n",
    "grouped = df.groupby('College')\n",
    "print(f\"Type: {type(grouped)}\")\n",
    "print(f\"Number of groups: {grouped.ngroups}\")\n",
    "print(f\"Groups: {list(grouped.groups.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations\n",
    "df.groupby('College')['GPA'].agg(['mean', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregate multiple columns with multiple functions\ndf.groupby('College').agg({\n    'GPA': ['mean', 'std'],\n    'Credits_Earned': ['mean', 'sum'],\n    'Study_Hours_Per_Week': 'mean',\n    'Student_ID': 'count'  # Count students\n})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "df.groupby(['College', 'Class_Year'])['GPA'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the multi-index output more readable\n",
    "df.groupby(['College', 'Class_Year'])['GPA'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by aggregated values\n",
    "df.groupby('Major')['GPA'].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Practice 7.4: GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the average Credits_Earned for each Class_Year\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find the number of students in each College and Class_Year combination\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Which Class_Year has the highest average GPA?\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4. Calculate mean GPA and mean Study_Hours_Per_Week for each Campus_Housing type\n# Your code:\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Creating New Columns\n",
    "\n",
    "You can create new columns from existing data using simple assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to avoid modifying the original\n",
    "df_work = df.copy()\n",
    "\n",
    "# New column: Completion Rate\n",
    "df_work['Completion_Rate'] = df_work['Credits_Earned'] / df_work['Credits_Attempted']\n",
    "df_work[['Student_ID', 'Credits_Attempted', 'Credits_Earned', 'Completion_Rate']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column: Quality Points (GPA * Credits_Earned)\n",
    "df_work['Quality_Points'] = df_work['GPA'] * df_work['Credits_Earned']\n",
    "df_work[['Student_ID', 'GPA', 'Credits_Earned', 'Quality_Points']].head()"
   ]
  },
  {
   "cell_type": "code",
   "source": "# New columns using the extended dataset variables\n# Study Efficiency: GPA per hour of study (higher = more efficient)\ndf_work['Study_Efficiency'] = (df_work['GPA'] / df_work['Study_Hours_Per_Week']).round(3)\n\n# Boolean column: Does the student have a part-time job?\ndf_work['Has_Job'] = df_work['Part_Time_Job'] != 'No'\n\ndf_work[['Student_ID', 'Study_Hours_Per_Week', 'GPA', 'Study_Efficiency', \n         'Part_Time_Job', 'Has_Job']].head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column based on conditions: Academic Standing\n",
    "# Dean's List: GPA >= 3.5\n",
    "# Good Standing: GPA >= 2.0\n",
    "# Probation: GPA < 2.0\n",
    "\n",
    "def get_standing(gpa):\n",
    "    if gpa >= 3.5:\n",
    "        return \"Dean's List\"\n",
    "    elif gpa >= 2.0:\n",
    "        return \"Good Standing\"\n",
    "    else:\n",
    "        return \"Probation\"\n",
    "\n",
    "df_work['Standing'] = df_work['GPA'].apply(get_standing)\n",
    "df_work['Standing'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative using np.select for multiple conditions (faster)\n",
    "conditions = [\n",
    "    df_work['GPA'] >= 3.5,\n",
    "    df_work['GPA'] >= 2.0,\n",
    "    df_work['GPA'] < 2.0\n",
    "]\n",
    "choices = [\"Dean's List\", \"Good Standing\", \"Probation\"]\n",
    "\n",
    "df_work['Standing2'] = np.select(conditions, choices)\n",
    "df_work['Standing2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.cut for binning continuous values\n",
    "df_work['GPA_Category'] = pd.cut(\n",
    "    df_work['GPA'],\n",
    "    bins=[0, 2.0, 2.5, 3.0, 3.5, 4.0],\n",
    "    labels=['Below 2.0', '2.0-2.5', '2.5-3.0', '3.0-3.5', '3.5-4.0']\n",
    ")\n",
    "df_work['GPA_Category'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.10 Handling Missing Data\n\nReal datasets often have missing values. Pandas represents these as `NaN` (Not a Number) or `None`.\n\nOur extended dataset actually has real missing values â€” the `First_Generation` column has blank entries for about 10% of students (this is common in survey data where some students don't respond). Let's start by examining those, then practice with synthetic missing values."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# First, check for real missing values in our extended dataset\nprint(\"Missing values per column:\")\nprint(df.isna().sum())\nprint(f\"\\nTotal missing: {df.isna().sum().sum()}\")\n\n# Note: First_Generation has blanks stored as empty strings, not NaN\n# Let's check both\nprint(f\"\\nFirst_Generation value counts (including blanks):\")\nprint(df['First_Generation'].value_counts(dropna=False))\n\n# Now create a copy with synthetic missing values to practice with\ndf_missing = df.copy()\n\n# Introduce some NaN values\ndf_missing.loc[5:10, 'GPA'] = np.nan\ndf_missing.loc[15:20, 'Credits_Earned'] = np.nan\n\nprint(f\"\\nAfter adding synthetic NaN values:\")\nprint(df_missing.isna().sum())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df_missing.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total missing values\n",
    "print(f\"Total missing: {df_missing.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with any missing value\n",
    "df_missing[df_missing.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "df_dropped = df_missing.dropna()\n",
    "print(f\"Original rows: {len(df_missing)}\")\n",
    "print(f\"After dropna: {len(df_dropped)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only rows where GPA is missing\n",
    "df_dropped_gpa = df_missing.dropna(subset=['GPA'])\n",
    "print(f\"After dropping missing GPA: {len(df_dropped_gpa)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a constant\n",
    "df_filled = df_missing.copy()\n",
    "df_filled['GPA'] = df_filled['GPA'].fillna(0)\n",
    "df_filled.loc[5:10, ['Student_ID', 'GPA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with column mean\n",
    "df_filled = df_missing.copy()\n",
    "mean_gpa = df_filled['GPA'].mean()\n",
    "df_filled['GPA'] = df_filled['GPA'].fillna(mean_gpa)\n",
    "print(f\"Filled missing GPAs with mean: {mean_gpa:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with forward fill (use previous value)\n",
    "df_filled = df_missing.copy()\n",
    "df_filled['GPA'] = df_filled['GPA'].ffill()\n",
    "df_filled.loc[3:12, ['Student_ID', 'GPA']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Merging and Concatenating DataFrames\n",
    "\n",
    "Often you need to combine data from multiple sources. Pandas provides `merge()` (like SQL JOIN) and `concat()` (stacking DataFrames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create sample DataFrames to demonstrate merging\n# College information\ncollege_info = pd.DataFrame({\n    'College': ['College of Business', 'College of Engineering', \n                'College of Arts and Sciences', 'College of Health',\n                'College of Education'],\n    'Dean': ['Dr. Smith', 'Dr. Johnson', 'Dr. Williams', 'Dr. Brown', 'Dr. Davis'],\n    'Founded': [1961, 1925, 1865, 1980, 1970]\n})\n\ncollege_info"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge student data with college info\n",
    "df_merged = df.merge(college_info, on='College', how='left')\n",
    "print(f\"Original columns: {df.columns.tolist()}\")\n",
    "print(f\"Merged columns: {df_merged.columns.tolist()}\")\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different merge types\n",
    "# 'inner': only rows with matching keys in both DataFrames\n",
    "# 'left': all rows from left DataFrame, matching from right\n",
    "# 'right': all rows from right DataFrame, matching from left\n",
    "# 'outer': all rows from both DataFrames\n",
    "\n",
    "# Example: students might be in colleges not in our college_info\n",
    "print(f\"Unique colleges in student data: {df['College'].nunique()}\")\n",
    "print(f\"Colleges in college_info: {len(college_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames (stacking)\n",
    "# Split students into two groups and recombine\n",
    "df_first_half = df.iloc[:300]\n",
    "df_second_half = df.iloc[300:]\n",
    "\n",
    "df_combined = pd.concat([df_first_half, df_second_half])\n",
    "print(f\"First half: {len(df_first_half)}\")\n",
    "print(f\"Second half: {len(df_second_half)}\")\n",
    "print(f\"Combined: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after concatenation\n",
    "df_combined = pd.concat([df_first_half, df_second_half], ignore_index=True)\n",
    "df_combined.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Practice 7.5: WALK Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2. Create a column 'At_Risk' that is True if GPA < 2.0 OR Completion_Rate < 0.85 OR Study_Hours_Per_Week < 5\n# Your code:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. For each College, calculate:\n#    - Number of students\n#    - Mean GPA\n#    - Mean Study_Hours_Per_Week\n#    - Number on Dean's List (GPA >= 3.5)\n# Your code:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4. Create a pivot table showing mean GPA by Campus_Housing (rows) and Class_Year (columns)\n# Hint: Use pd.pivot_table() or groupby().unstack()\n# Your code:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a pivot table showing mean GPA by College (rows) and Class_Year (columns)\n",
    "# Hint: Use pd.pivot_table() or groupby().unstack()\n",
    "# Your code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸŽ¯ Project: Crestview Student Success Analysis\n\nCreate a comprehensive analysis of the **extended Crestview student dataset** (600 students, 15 variables). Your analysis should answer key questions about student success patterns and provide actionable insights.\n\nYou now have access to variables that let you ask much richer questions:\n- **Study_Hours_Per_Week**: Does study time predict GPA?\n- **Campus_Housing**: Do on-campus students perform differently?\n- **Financial_Aid**: Is financial support associated with outcomes?\n- **Extracurriculars**: Do involved students have different GPAs?\n- **Part_Time_Job**: Does working affect academic performance?\n- **First_Generation**: Do first-gen students face different challenges?\n- **Distance_From_Home**: Does distance affect engagement?\n\n### Part 1: Data Preparation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\n\n# Load the extended dataset\ndf = pd.read_csv('data/crestview_students_extended.csv')\n\n# Create derived columns:\n# - Completion_Rate: Credits_Earned / Credits_Attempted\n# - Quality_Points: GPA * Credits_Earned\n# - Standing: Dean's List / Good Standing / Probation\n# - Credits_Remaining: 130 - Credits_Earned\n# - Study_Efficiency: GPA / Study_Hours_Per_Week\n# - Has_Job: True if Part_Time_Job != \"No\"\n# - At_Risk: GPA < 2.0 OR Completion_Rate < 0.85 OR Study_Hours_Per_Week < 5\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\n# Load the extended dataset\ndf = pd.read_csv('data/crestview_students_extended.csv')\n\n# Create derived columns\n# - Completion_Rate: Credits_Earned / Credits_Attempted\n# - Quality_Points: GPA * Credits_Earned\n# - Standing: Dean's List / Good Standing / Probation\n# - Credits_Remaining: 130 - Credits_Earned\n# - Study_Efficiency: GPA / Study_Hours_Per_Week\n# - Has_Job: True if Part_Time_Job != \"No\"\n# - At_Risk: GPA < 2.0 OR Completion_Rate < 0.85 OR Study_Hours_Per_Week < 5\n\n# Your implementation:\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary statistics table for all numeric columns\n",
    "# Include: count, mean, std, min, 25%, 50%, 75%, max\n",
    "\n",
    "# Your implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency tables for categorical variables\n",
    "# - College distribution\n",
    "# - Class Year distribution\n",
    "# - Standing distribution\n",
    "\n",
    "# Your implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Compare performance across colleges\n# For each college, show:\n# - Number of students\n# - Mean GPA\n# - Mean Study_Hours_Per_Week\n# - Percentage on Dean's List\n# - Percentage on Probation\n# - Mean Completion Rate\n# - Percentage with Financial Aid\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare performance across class years\n# Do seniors have higher GPAs than freshmen?\n# How does completion rate vary by class year?\n# Do study hours change as students progress?\n# Does part-time employment differ by class year?\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create pivot tables:\n# 1. Mean GPA by College and Class Year\n# 2. Mean Study_Hours by Campus_Housing and Class Year\n# 3. Cross-tabulation of Financial_Aid by College\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table: Mean GPA by College and Class Year\n",
    "\n",
    "# Your implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Define at-risk criteria:\n# 1. GPA < 2.0 (academic probation)\n# 2. Completion Rate < 85%\n# 3. Study_Hours_Per_Week < 5 (minimal study effort)\n# 4. Seniors with GPA < 2.5 (graduation risk)\n# 5. GPA more than 1.5 std deviations below the mean\n\n# Identify students meeting ANY of these criteria\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze the at-risk population\n# - How many students are at risk?\n# - What's the breakdown by college?\n# - What's the breakdown by class year?\n# - What's the average GPA of at-risk students?\n# - What's the average Study_Hours_Per_Week of at-risk vs non-at-risk?\n# - What's the Campus_Housing distribution for at-risk students?\n# - Are first-generation students overrepresented in the at-risk group?\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the at-risk population\n",
    "# - How many students are at risk?\n",
    "# - What's the breakdown by college?\n",
    "# - What's the breakdown by class year?\n",
    "# - What's the average GPA of at-risk students?\n",
    "\n",
    "# Your implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Find the top 10 students by GPA\n# Show their Student_ID, College, Major, GPA, Study_Hours_Per_Week, and Campus_Housing\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the top major by average GPA (minimum 10 students)\n# Also show mean Study_Hours and % with Financial_Aid for each qualifying major\n# Why is a minimum count important?\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Profile high achievers (GPA >= 3.8):\n# - What colleges and majors are they in?\n# - How many hours do they study per week on average?\n# - What percentage have part-time jobs?\n# - What's their campus housing distribution?\n# - Are first-generation students represented among high achievers?\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find students with perfect 4.0 GPA\n",
    "# What colleges and majors are they in?\n",
    "\n",
    "# Your implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create a comprehensive summary report\n# Include:\n# - Overall dataset summary (600 students, 15 variables)\n# - Key statistics by college (GPA, study hours, financial aid)\n# - Key statistics by class year (progression patterns)\n# - Study hours vs GPA correlation\n# - At-risk student summary with contributing factors\n# - Top performer highlights and their characteristics\n# - Campus housing and financial aid patterns\n# - 5 key insights or recommendations for university administration\n\n# Your implementation:\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary report\n",
    "# Include:\n",
    "# - Overall dataset summary\n",
    "# - Key statistics by college\n",
    "# - Key statistics by class year\n",
    "# - At-risk student summary\n",
    "# - Top performer highlights\n",
    "# - 3-5 key insights or recommendations\n",
    "\n",
    "# Your implementation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Project Reflection\n\n1. What pandas operations were most useful for this analysis?\n2. How does the pandas approach compare to the NumPy approach from Chapter 6?\n3. Which of the 15 variables provided the most interesting insights? Why?\n4. What questions came up that you couldn't answer with this data?\n5. How did you use AI during this project? What did you learn from those interactions?\n6. What additional data would help you provide better insights?"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Reflection\n",
    "\n",
    "1. What pandas operations were most useful for this analysis?\n",
    "2. How does the pandas approach compare to the NumPy approach from Chapter 6?\n",
    "3. What questions came up that you couldn't answer with this data?\n",
    "4. How did you use AI during this project? What did you learn from those interactions?\n",
    "5. What additional data would help you provide better insights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Accountability Check\n\n## ðŸ› CRAWL (Must do without AI)\n- [ ] Load CSV files with `pd.read_csv()`\n- [ ] Inspect DataFrames: `head()`, `info()`, `describe()`, `shape`\n- [ ] Select columns: `df['col']` and `df[['col1', 'col2']]`\n- [ ] Explain the difference between `loc[]` and `iloc[]`\n- [ ] Filter rows with boolean conditions\n- [ ] Combine conditions with `&` and `|` (with parentheses!)\n- [ ] Sort with `sort_values()`\n- [ ] Use basic aggregations: `mean()`, `sum()`, `count()`, `value_counts()`\n\n## ðŸš¶ WALK (AI to learn, write code yourself)\n- [ ] Use `groupby()` for split-apply-combine operations\n- [ ] Create new columns from existing data (including Study_Efficiency, Has_Job)\n- [ ] Apply functions to columns with `apply()`\n- [ ] Handle missing values: `isna()`, `dropna()`, `fillna()`\n- [ ] Merge DataFrames with `merge()`\n- [ ] Concatenate DataFrames with `concat()`\n- [ ] Create pivot tables with categorical variables (Campus_Housing, Financial_Aid)\n\n## ðŸš€ RUN (AI-assisted, must understand)\n- [ ] Build complete analysis pipelines using all 15 variables\n- [ ] Generate summary reports with multi-dimensional groupby\n- [ ] Export results to CSV\n- [ ] Identify patterns across study hours, housing, financial aid, and outcomes\n- [ ] Create derived metrics and categories\n- [ ] Profile at-risk and high-performing student groups\n\n**Review CRAWL material if you can't do it from memory. This will be on the final exam.**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## What's Next?\n\nIn **Chapter 8: Data Visualization**, you'll learn:\n\n- **Matplotlib:** The foundational plotting library\n- **Seaborn:** Statistical visualizations built on matplotlib\n- **Plot Types:** Bar charts, histograms, scatter plots, box plots, heatmaps\n- **Customization:** Labels, titles, colors, styles\n- **Telling Stories:** Choosing the right visualization for your data\n\nAll the analysis you did in this chapter becomes much more powerful when visualized. Numbers tell you what happened; visualizations show you why it matters.\n\n**Preview:** With 15 variables, you can now create rich, multi-dimensional visualizations:\n```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Study Hours vs GPA, colored by Campus Housing\nsns.scatterplot(data=df, x='Study_Hours_Per_Week', y='GPA', hue='Campus_Housing')\nplt.title('Do Study Hours Predict GPA? Does Housing Matter?')\nplt.show()\n```\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In **Chapter 8: Data Visualization**, you'll learn:\n",
    "\n",
    "- **Matplotlib:** The foundational plotting library\n",
    "- **Seaborn:** Statistical visualizations built on matplotlib\n",
    "- **Plot Types:** Bar charts, histograms, scatter plots, box plots\n",
    "- **Customization:** Labels, titles, colors, styles\n",
    "- **Telling Stories:** Choosing the right visualization for your data\n",
    "\n",
    "All the analysis you did in this chapter becomes much more powerful when visualized. Numbers tell you what happened; visualizations show you why it matters.\n",
    "\n",
    "**Preview:** A taste of what's coming:\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GPA distribution by college\n",
    "sns.boxplot(data=df, x='College', y='GPA')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('GPA Distribution by College')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}